{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZAD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random as rnd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from itertools import permutations\n",
    "from functools import reduce\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ\\-]+')\n",
    "    percent_train = 0.9\n",
    "    normalized_data = [[word.lower() for word in re.findall(RE, line)] for line in data]\n",
    "    normalized_data = [superbase_mapping(sentence) for sentence in normalized_data if sentence]\n",
    "    randomized = normalized_data.copy()\n",
    "    rng = rnd.Random()\n",
    "    rng.shuffle(randomized)\n",
    "    train_data = randomized[:int(percent_train * len(randomized))]\n",
    "    validate_data = randomized[-int((1.0 - percent_train) * len(randomized)):]\n",
    "    return train_data, validate_data, randomized, normalized_data\n",
    "\n",
    "def calculate_chance_of_unknown(train, validate):\n",
    "    dict_words = {}\n",
    "    for line in train:\n",
    "        for word in line:\n",
    "            if word not in dict_words:\n",
    "                dict_words[word] = 0\n",
    "            dict_words[word] += 1\n",
    "    not_found = 0\n",
    "    count = 0\n",
    "    for line in validate:\n",
    "        for word in line:\n",
    "            count += 1\n",
    "            if word not in dict_words:\n",
    "                not_found += 1\n",
    "    return(not_found / count)\n",
    "\n",
    "def calculate_corpus_sentence(file):\n",
    "    with open (file) as corpus:\n",
    "        data = corpus.readlines()\n",
    "    train_data, validate_data, random, normalized_data = generate_data(data)\n",
    "    bgram = digram(normalized_data)\n",
    "    tgram = trigram(normalized_data)\n",
    "    chance = calculate_chance_of_unknown(train_data, validate_data)\n",
    "    dict_word = {}\n",
    "    dict_character = {}\n",
    "    flat = [item for sublist in normalized_data for item in sublist]\n",
    "    for w in flat:\n",
    "        if w not in dict_word:\n",
    "            dict_word[w] = 0\n",
    "        dict_word[w] += 1\n",
    "        for c in w:\n",
    "            if c not in dict_character:\n",
    "                dict_character[c] = 0\n",
    "            dict_character[c] += 1\n",
    "    for k in dict_word:\n",
    "        dict_word[k] = np.log(dict_word[k] / len(flat))\n",
    "    char_num = sum([dict_character[k] for k in dict_character])\n",
    "    for k in dict_character:\n",
    "        dict_character[k] = np.log(dict_character[k] / char_num)\n",
    "    return dict_character, dict_word, chance, bgram, tgram\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_dict = {}\n",
    "def superbase_mapping(word_list):\n",
    "    if len(super_dict) == 0:\n",
    "        with open('../train data/superbazy.txt') as bazy:\n",
    "            for line in bazy:\n",
    "                w1, w2 = (line.split(' ')[0], line.split(' ')[1])\n",
    "                super_dict[w1] = w2.strip()\n",
    "    return [super_dict[w] if w in super_dict else w for w in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digram(text):\n",
    "    grams = defaultdict(float)\n",
    "    total = 0\n",
    "    for line in text:\n",
    "        if not line:\n",
    "            continue\n",
    "        start = line[0]\n",
    "        for w1, w2 in zip(line, line[1:]):\n",
    "            grams[(w1, w2)] += 1\n",
    "            total += 1\n",
    "            start = word\n",
    "    \n",
    "    for gram in grams:\n",
    "        grams[gram] = np.log(grams[gram] / total)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram(text):\n",
    "    grams = defaultdict(float)\n",
    "    total = 0\n",
    "    for line in text:\n",
    "        if not line:\n",
    "            continue\n",
    "        for w1, w2, w3 in zip(line, line[1:], line[2:]):\n",
    "            grams[(w1, w2, w3)] += 1\n",
    "            total += 1\n",
    "            start = word\n",
    "    \n",
    "    for gram in grams:\n",
    "        grams[gram] = np.log(grams[gram] / total)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ochar_dict, oword_dict, ochance_of_unknown, odig, otri = calculate_corpus_sentence('../train data/dane_pozytywistyczne/korpus_orzeszkowej.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pchar_dict, pword_dict, pchance_of_unknown, pdig, ptri = calculate_corpus_sentence('../train data/dane_pozytywistyczne/korpus_prusa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schar_dict, sword_dict, schance_of_unknown, sdig, stri = calculate_corpus_sentence('../train data/dane_pozytywistyczne/korpus_sienkiewicza.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_test(test_data, A=0.2, B=0.3, C=0.4):\n",
    "        Names = {\n",
    "            'Orzeszkowa': (ochar_dict, oword_dict, odig, otri),\n",
    "            'Prus' : (pchar_dict, pword_dict, pdig, ptri),\n",
    "            'Sienkiewicz': (schar_dict, sword_dict, sdig, stri)\n",
    "        }\n",
    "        D = 1 - A - B - C\n",
    "        char_prop = defaultdict(float)\n",
    "        word_prop = defaultdict(float)\n",
    "        dig_prop = defaultdict(float)\n",
    "        tri_prop = defaultdict(float)\n",
    "        for name, data in Names.items():\n",
    "            char_dict, word_dict, dig, tri = data\n",
    "            for sentence in test_data:\n",
    "                for w1, w2, w3 in zip(sentence, sentence[1:], sentence[2:]):\n",
    "                    c_prop = 0\n",
    "                    for c in w1:\n",
    "                        c_prop += D * char_dict[c] if c in char_dict else np.log(1/1000000)\n",
    "                    char_prop[name] += c_prop / len(w1)\n",
    "                    word_prop[name] += word_dict[w1] if w1 in word_dict else np.log(1/110000)\n",
    "                    dig_prop[name] += dig[(w1, w2)] if (w1, w2) in dig else np.log(1/100000)\n",
    "                    tri_prop[name] += tri[(w1, w2, w3)] if (w1, w2, w3) in tri else np.log(1/50000)\n",
    "        for name in Names:\n",
    "            char_prop[name] = -1 / char_prop[name]\n",
    "            word_prop[name] = -1 / word_prop[name]                  \n",
    "            dig_prop[name] = -1 / dig_prop[name]\n",
    "            tri_prop[name] = -1 / tri_prop[name]\n",
    "\n",
    "        s_char = sum(char_prop.values())\n",
    "        s_word = sum(word_prop.values())\n",
    "        s_dig = sum(dig_prop.values())\n",
    "        s_tri = sum(tri_prop.values())\n",
    "        \n",
    "        s_sum = s_char + s_word + s_dig + s_tri\n",
    "            \n",
    "        for name in Names:\n",
    "            char_prop[name] *= 1 / s_char\n",
    "            word_prop[name] *= 1 / s_word\n",
    "            dig_prop[name] *= 1 / s_dig\n",
    "            tri_prop[name] *= 1 / s_tri\n",
    "        \n",
    "        results = {}\n",
    "        for author in Names:\n",
    "            results[author] = (s_char / s_sum) * char_prop[author] + (s_word / s_sum) * word_prop[author] + (s_dig / s_sum) * dig_prop[author] + (s_tri / s_sum) * dig_prop[author]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test_orzeszkowej.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej1.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej11.txt', 'Sienkiewicz'),\n",
      " ('test_orzeszkowej13.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej15.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej17.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej19.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej21.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej3.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej5.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej7.txt', 'Orzeszkowa'),\n",
      " ('test_orzeszkowej9.txt', 'Orzeszkowa'),\n",
      " ('test_prusa0.txt', 'Sienkiewicz'),\n",
      " ('test_prusa10.txt', 'Prus'),\n",
      " ('test_prusa12.txt', 'Prus'),\n",
      " ('test_prusa14.txt', 'Prus'),\n",
      " ('test_prusa16.txt', 'Prus'),\n",
      " ('test_prusa18.txt', 'Prus'),\n",
      " ('test_prusa2.txt', 'Prus'),\n",
      " ('test_prusa20.txt', 'Prus'),\n",
      " ('test_prusa22.txt', 'Prus'),\n",
      " ('test_prusa24.txt', 'Prus'),\n",
      " ('test_prusa26.txt', 'Prus'),\n",
      " ('test_prusa28.txt', 'Prus'),\n",
      " ('test_prusa30.txt', 'Prus'),\n",
      " ('test_prusa32.txt', 'Prus'),\n",
      " ('test_prusa34.txt', 'Prus'),\n",
      " ('test_prusa36.txt', 'Prus'),\n",
      " ('test_prusa38.txt', 'Prus'),\n",
      " ('test_prusa4.txt', 'Prus'),\n",
      " ('test_prusa40.txt', 'Prus'),\n",
      " ('test_prusa6.txt', 'Prus'),\n",
      " ('test_prusa8.txt', 'Prus'),\n",
      " ('test_sienkiewicza1.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza11.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza13.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza15.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza17.txt', 'Prus'),\n",
      " ('test_sienkiewicza19.txt', 'Prus'),\n",
      " ('test_sienkiewicza21.txt', 'Prus'),\n",
      " ('test_sienkiewicza23.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza25.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza27.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza29.txt', 'Prus'),\n",
      " ('test_sienkiewicza3.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza31.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza33.txt', 'Prus'),\n",
      " ('test_sienkiewicza35.txt', 'Prus'),\n",
      " ('test_sienkiewicza37.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza39.txt', 'Prus'),\n",
      " ('test_sienkiewicza41.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza43.txt', 'Prus'),\n",
      " ('test_sienkiewicza45.txt', 'Prus'),\n",
      " ('test_sienkiewicza47.txt', 'Prus'),\n",
      " ('test_sienkiewicza49.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza5.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza51.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza53.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza7.txt', 'Sienkiewicz'),\n",
      " ('test_sienkiewicza9.txt', 'Prus')]\n"
     ]
    }
   ],
   "source": [
    "RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ\\-]+')\n",
    "res = []\n",
    "for path in glob.glob('../train data/dane_pozytywistyczne/testy1/*'):\n",
    "    filename = path.split('/')[-1]\n",
    "    with open(path, 'rt') as test:\n",
    "        normalized_data = [[word.lower() for word in re.findall(RE, line)] for line in test]\n",
    "        normalized_data = [superbase_mapping(sentence) for sentence in normalized_data if sentence]\n",
    "    output = naive_bayes_test(normalized_data, A=0.25, B=0.3, C=0.35)\n",
    "    res.append((filename, max(output.items(), key=lambda t: t[1])[0]))\n",
    "res.sort(key=lambda t: t[0])\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 60)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum([1 for x in res if x[0].split('_')[1][0] == x[1].lower()[0]]), len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "zad2_bigram_dict = {}\n",
    "tag_dict = defaultdict(dict)\n",
    "tag_mapper= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_of(list_of_lists):\n",
    "    words = set(reduce(list.__add__, list_of_lists))\n",
    "    dict = {}\n",
    "    with open('../train data/2grams', 'r', encoding='utf8') as bigram:\n",
    "        for line in bigram:\n",
    "            spl = [l for l in line.strip().lower().split(' ') if l]\n",
    "            occ, w1, w2 = int(spl[0]), spl[1], spl[2]\n",
    "            if w1 not in words:\n",
    "                continue\n",
    "            if w1 in dict:\n",
    "                _, l, _ = dict[w1][-1]\n",
    "                dict[w1].append((l+1, occ + l, w2))\n",
    "            else:\n",
    "                dict[w1] = [(0, occ, w2)]\n",
    "    return dict\n",
    "\n",
    "def zad2(a=0.1):\n",
    "    def perm_heu(permutation_list, dict):\n",
    "        res = []\n",
    "        mapped = permutation_list#superbase_mapping(permutation_list)\n",
    "        for w1, w2 in zip(mapped, mapped[1:]):\n",
    "            if not w1 in dict:\n",
    "                continue\n",
    "            else:\n",
    "                asc = dict[w1]\n",
    "                score = -0.1\n",
    "                _, last, _ = asc[-1]\n",
    "                for lr, hr, w in asc:\n",
    "                    if w == w2:\n",
    "                        score = (hr - lr + 1) / last\n",
    "                res.append(score)\n",
    "        return (' '.join(permutation_list), sum(res))\n",
    "    \n",
    "    def tag_heuristic(perm, dict):\n",
    "        global tag_dict\n",
    "        global tag_mapper\n",
    "        #perm = superbase_mapping(perm)\n",
    "        if not tag_dict:\n",
    "            with open('../train data/supertags.txt', 'r', encoding='utf8') as supertags:\n",
    "                for line in supertags:\n",
    "                    line = line.strip()\n",
    "                    tag_mapper[line.split(' ')[0].lower()] = line.split(' ')[1].strip()\n",
    "            for k in dict:\n",
    "                if k not in tag_mapper:\n",
    "                    continue\n",
    "                for lr, hr, w in dict[k]:\n",
    "                    if w in tag_mapper:\n",
    "                        ktag = tag_mapper[k]\n",
    "                        if tag_mapper[w] not in tag_dict[ktag]:\n",
    "                            tag_dict[ktag][tag_mapper[w]] = 0\n",
    "                        tag_dict[ktag][tag_mapper[w]] += (hr - lr) + 1\n",
    "        res = [-0.1]\n",
    "        for w1, w2 in zip(perm, perm[1:]):\n",
    "            if w1 not in tag_mapper or w2 not in tag_mapper:\n",
    "                continue\n",
    "            tag1 = tag_mapper[w1]\n",
    "            tag2 = tag_mapper[w2]\n",
    "            if tag2 not in tag_dict[tag1]:\n",
    "                continue\n",
    "            total = sum(tag_dict[tag1].values())\n",
    "            res.append(tag_dict[tag1][tag2] / total)\n",
    "        return sum(res)\n",
    "\n",
    "    def heu_interp(h1, h2):\n",
    "        return h1[0], a * h2 + (1 - a) * h1[1]\n",
    "    \n",
    "    test_data = [\n",
    "        'Judyta dała wczoraj Stefanowi czekoladki',\n",
    "        'Babuleńka miała dwa rogate koziołki',\n",
    "        'Wczoraj wieczorem spotkałem pewną piękną kobietę',\n",
    "        'Dlaczego zawsze mam największego pecha',\n",
    "        'Po raz pierwszy zauważyłem ten szczegół',\n",
    "        'Zjadłem pyszną kanapkę z szynką i serem'\n",
    "    ]\n",
    "    results = []\n",
    "    global zad2_bigram_dict\n",
    "    if len(zad2_bigram_dict) == 0:\n",
    "        zad2_bigram_dict = generate_bigram_of([test.lower().split(' ') for test in test_data])\n",
    "    for test in test_data:\n",
    "        results.append((test, [heu_interp(perm_heu(perm, zad2_bigram_dict), tag_heuristic(perm, zad2_bigram_dict)) for perm in permutations(test.lower().split(' '))]))\n",
    "\n",
    "    for test, perm_list in results:\n",
    "        perm_list.sort(reverse=True, key=lambda x: x[1])\n",
    "        print(u'test: {0}\\n1* - {1}\\n2* - {2}\\n3* - {3}\\n'.format(test, perm_list[0], perm_list[1], perm_list[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('dała wczoraj stefanowi czekoladki judyta', -0.022933789763080453)\n",
      "2* - ('wczoraj stefanowi dała czekoladki judyta', -0.022933789763080453)\n",
      "3* - ('dała czekoladki wczoraj stefanowi judyta', -0.02633515030729814)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('miała rogate koziołki dwa babuleńka', -0.09977864842729708)\n",
      "2* - ('miała koziołki rogate dwa babuleńka', -0.09977864842729708)\n",
      "3* - ('rogate miała koziołki dwa babuleńka', -0.09977864842729708)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('wczoraj kobietę wieczorem pewną spotkałem piękną', 0.014881529456569658)\n",
      "2* - ('wczoraj kobietę wieczorem pewną piękną spotkałem', 0.014881529456569658)\n",
      "3* - ('wczoraj kobietę wieczorem piękną spotkałem pewną', 0.014881529456569658)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('mam pecha dlaczego zawsze największego', -0.03230853756627626)\n",
      "2* - ('pecha dlaczego zawsze największego mam', -0.03230853756627626)\n",
      "3* - ('mam największego pecha dlaczego zawsze', -0.032602153631074424)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten raz pierwszy szczegół po', 0.6601941258288516)\n",
      "2* - ('ten raz pierwszy szczegół po zauważyłem', 0.6102074993620351)\n",
      "3* - ('zauważyłem po ten raz pierwszy szczegół', 0.557129601362793)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem kanapkę i z szynką serem pyszną', 0.03973652109564718)\n",
      "2* - ('kanapkę i z szynką serem pyszną zjadłem', 0.03973652109564718)\n",
      "3* - ('zjadłem kanapkę serem pyszną szynką i z', 0.03932419522764667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('dała wczoraj stefanowi czekoladki judyta', -0.004554568032084956)\n",
      "2* - ('wczoraj stefanowi dała czekoladki judyta', -0.004554568032084956)\n",
      "3* - ('dała czekoladki wczoraj stefanowi judyta', -0.005234840140928495)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('miała rogate koziołki dwa babuleńka', -0.09995572968545943)\n",
      "2* - ('miała koziołki rogate dwa babuleńka', -0.09995572968545943)\n",
      "3* - ('rogate miała koziołki dwa babuleńka', -0.09995572968545943)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('wczoraj kobietę wieczorem pewną spotkałem piękną', 0.003056780692641766)\n",
      "2* - ('wczoraj kobietę wieczorem pewną piękną spotkałem', 0.003056780692641766)\n",
      "3* - ('wczoraj kobietę wieczorem piękną spotkałem pewną', 0.003056780692641766)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('mam pecha dlaczego zawsze największego', -0.005450433490513594)\n",
      "2* - ('pecha dlaczego zawsze największego mam', -0.005450433490513594)\n",
      "3* - ('mam największego pecha dlaczego zawsze', -0.005580360220926988)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten szczegół po raz pierwszy', 0.32180051044827324)\n",
      "2* - ('zauważyłem ten raz pierwszy szczegół po', 0.30776721668973095)\n",
      "3* - ('zauważyłem po raz pierwszy szczegół ten', 0.29283510275816116)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem kanapkę i z szynką serem pyszną', 0.01469163971348406)\n",
      "2* - ('kanapkę i z szynką serem pyszną zjadłem', 0.01469163971348406)\n",
      "3* - ('zjadłem kanapkę serem pyszną szynką i z', 0.014608405105843571)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('judyta czekoladki stefanowi dała wczoraj', -0.03469387755102041)\n",
      "2* - ('dała judyta czekoladki stefanowi wczoraj', -0.03469387755102041)\n",
      "3* - ('dała czekoladki judyta stefanowi wczoraj', -0.03469387755102041)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('miała rogate koziołki dwa babuleńka', -0.09964583748367534)\n",
      "2* - ('miała koziołki rogate dwa babuleńka', -0.09964583748367534)\n",
      "3* - ('rogate miała koziołki dwa babuleńka', -0.09964583748367534)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('wczoraj kobietę wieczorem pewną spotkałem piękną', 0.02375009102951558)\n",
      "2* - ('wczoraj kobietę wieczorem pewną piękną spotkałem', 0.02375009102951558)\n",
      "3* - ('wczoraj kobietę wieczorem piękną spotkałem pewną', 0.02375009102951558)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('mam pecha dlaczego zawsze największego', -0.05245211562309827)\n",
      "2* - ('pecha dlaczego zawsze największego mam', -0.05245211562309827)\n",
      "3* - ('mam największego pecha dlaczego zawsze', -0.052868498688685)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten raz pierwszy szczegół po', 0.9245143076831922)\n",
      "2* - ('ten raz pierwszy szczegół po zauważyłem', 0.9045357053362857)\n",
      "3* - ('zauważyłem ten szczegół pierwszy raz po', 0.8827410671679297)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem z szynką i kanapkę serem pyszną', 0.0616469629698419)\n",
      "2* - ('z szynką i kanapkę serem pyszną zjadłem', 0.0616469629698419)\n",
      "3* - ('zjadłem kanapkę i szynką serem pyszną z', 0.06098811866395906)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('judyta dała wczoraj czekoladki stefanowi', -0.13278001633454298)\n",
      "2* - ('stefanowi judyta dała wczoraj czekoladki', -0.13278001633454298)\n",
      "3* - ('czekoladki judyta dała wczoraj stefanowi', -0.13392238135041507)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('dwa rogate miała babuleńka koziołki', -0.0861065307630569)\n",
      "2* - ('dwa rogate koziołki miała babuleńka', -0.0861065307630569)\n",
      "3* - ('koziołki dwa rogate miała babuleńka', -0.0861065307630569)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('wczoraj wieczorem spotkałem pewną piękną kobietę', 0.21896871047705163)\n",
      "2* - ('pewną piękną kobietę wieczorem spotkałem wczoraj', 0.21861016962370375)\n",
      "3* - ('spotkałem pewną piękną kobietę wieczorem wczoraj', 0.21615512132709372)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('dlaczego zawsze mam największego pecha', 0.05095256227126561)\n",
      "2* - ('mam dlaczego zawsze największego pecha', 0.01400291690630695)\n",
      "3* - ('największego pecha mam dlaczego zawsze', 0.013988681828227545)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten raz pierwszy szczegół po', 0.6629411437170003)\n",
      "2* - ('zauważyłem po ten raz pierwszy szczegół', 0.6176831830919001)\n",
      "3* - ('ten raz pierwszy szczegół po zauważyłem', 0.6101956117769832)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem pyszną kanapkę z szynką i serem', 0.7197168975364687)\n",
      "2* - ('pyszną kanapkę z szynką i serem zjadłem', 0.7152880704859905)\n",
      "3* - ('serem pyszną kanapkę z szynką i zjadłem', 0.7122221130954756)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('stefanowi judyta dała wczoraj czekoladki', -0.1827959166646358)\n",
      "2* - ('judyta dała wczoraj czekoladki stefanowi', -0.18279591666463582)\n",
      "3* - ('czekoladki judyta dała wczoraj stefanowi', -0.1830243896678102)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('babuleńka miała dwa rogate koziołki', -0.17620666599170376)\n",
      "2* - ('koziołki babuleńka miała dwa rogate', -0.17620666599170376)\n",
      "3* - ('babuleńka koziołki miała dwa rogate', -0.17688693810054731)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('wczoraj wieczorem spotkałem pewną piękną kobietę', 0.06486619248120291)\n",
      "2* - ('pewną piękną kobietę wieczorem spotkałem wczoraj', 0.052846353877496824)\n",
      "3* - ('spotkałem pewną piękną kobietę wieczorem wczoraj', 0.05181390941594892)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('dlaczego zawsze mam największego pecha', 0.014037428121282267)\n",
      "2* - ('największego pecha dlaczego mam zawsze', -0.07130263917704426)\n",
      "3* - ('dlaczego mam zawsze największego pecha', -0.07167786399507677)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten szczegół po raz pierwszy', 0.3249490043442915)\n",
      "2* - ('ten szczegół po raz pierwszy zauważyłem', 0.3224429921210713)\n",
      "3* - ('zauważyłem po raz pierwszy szczegół ten', 0.31274309005914824)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem pyszną kanapkę z szynką i serem', 0.5381643640304514)\n",
      "2* - ('pyszną kanapkę z szynką i serem zjadłem', 0.5372785986203557)\n",
      "3* - ('serem pyszną kanapkę z szynką i zjadłem', 0.5366646571237834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Judyta dała wczoraj Stefanowi czekoladki\n",
      "1* - ('judyta dała wczoraj czekoladki stefanowi', -0.09526809108697334)\n",
      "2* - ('stefanowi judyta dała wczoraj czekoladki', -0.09526809108697334)\n",
      "3* - ('judyta dała wczoraj stefanowi czekoladki', -0.09709587511236872)\n",
      "\n",
      "test: Babuleńka miała dwa rogate koziołki\n",
      "1* - ('dwa rogate miała babuleńka koziołki', -0.01778379595167186)\n",
      "2* - ('dwa rogate koziołki miała babuleńka', -0.01778379595167186)\n",
      "3* - ('koziołki dwa rogate miała babuleńka', -0.01778379595167186)\n",
      "\n",
      "test: Wczoraj wieczorem spotkałem pewną piękną kobietę\n",
      "1* - ('pewną piękną kobietę wieczorem spotkałem wczoraj', 0.34293303143335896)\n",
      "2* - ('wieczorem wczoraj spotkałem pewną piękną kobietę', 0.3394502021434506)\n",
      "3* - ('spotkałem pewną piękną kobietę wieczorem wczoraj', 0.33941103026045233)\n",
      "\n",
      "test: Dlaczego zawsze mam największego pecha\n",
      "1* - ('mam dlaczego zawsze największego pecha', 0.08075399698578717)\n",
      "2* - ('największego pecha mam dlaczego zawsze', 0.08073122086086013)\n",
      "3* - ('dlaczego zawsze mam największego pecha', 0.07863891288375313)\n",
      "\n",
      "test: Po raz pierwszy zauważyłem ten szczegół\n",
      "1* - ('zauważyłem ten raz pierwszy szczegół po', 0.9269602185654386)\n",
      "2* - ('ten raz pierwszy szczegół po zauważyłem', 0.9045166852002027)\n",
      "3* - ('zauważyłem ten szczegół pierwszy raz po', 0.8851869780501761)\n",
      "\n",
      "test: Zjadłem pyszną kanapkę z szynką i serem\n",
      "1* - ('zjadłem pyszną kanapkę z szynką i serem', 0.8558812976659819)\n",
      "2* - ('pyszną kanapkę z szynką i serem zjadłem', 0.8487951743852167)\n",
      "3* - ('serem pyszną kanapkę z szynką i zjadłem', 0.8438902050742447)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zad2(a=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unigram(file='../train data/1grams'):\n",
    "    dictionary = {}\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().lower()\n",
    "            line = line.split(' ')\n",
    "            dictionary[line[1]] = int(line[0])\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def load_tags(file='../train data/supertags.txt'):\n",
    "    tags = {}\n",
    "    with open(file, 'r', encoding='utf8') as t:\n",
    "        for line in t:\n",
    "            key, value = line.split()\n",
    "            tags[key.lower()] = value.lower()\n",
    "\n",
    "    return tags\n",
    "\n",
    "def tag_mapper(tags, uni):\n",
    "    dict = defaultdict(list)\n",
    "    for word in tags:\n",
    "        weight = 1\n",
    "        if word in uni:\n",
    "            weight += uni[word]\n",
    "        dict[tags[word]].append((word, weight))\n",
    "    return dict\n",
    "\n",
    "def find_tag(mapper, w):\n",
    "    suffix = w if len(w) <= 2 else w[-2:]\n",
    "    res = 't100'\n",
    "    best = 0\n",
    "    for tag in mapper:\n",
    "        cnt = 0\n",
    "        ttl = 0\n",
    "        for word, _ in mapper[tag]:\n",
    "            suffix2 = word if len(word) <= 2 else w[-2:]\n",
    "            if suffix == suffix2:\n",
    "                cnt += 1\n",
    "            ttl += 1\n",
    "        if cnt / ttl > best:\n",
    "            res = tag\n",
    "            best = cnt / ttl\n",
    "    return res\n",
    "\n",
    "def swap_uni(seq, tags, mapper):\n",
    "    res = []\n",
    "    for w in seq:\n",
    "        upper = w[0].isupper()\n",
    "        w = w.lower()\n",
    "        if w in tags:\n",
    "            tag = tags[w]\n",
    "        else:\n",
    "            tag = find_tag(mapper, w)\n",
    "        count = sum([i for _, i in mapper[tag]])\n",
    "        pick = rnd.randint(0, count)\n",
    "        new_word = ''\n",
    "        tmp = 0\n",
    "        for e, w in mapper[tag]:\n",
    "            tmp += w\n",
    "            if pick <= tmp:\n",
    "                new_word = e\n",
    "                break\n",
    "        res.append(new_word if not upper else new_word.capitalize())\n",
    "    return res\n",
    "\n",
    "zad3_tags = load_tags()\n",
    "zad3_uni = load_unigram()\n",
    "zad3_mapper = tag_mapper(zad3_tags, zad3_uni)\n",
    "\n",
    "def test_uniswap(tests):\n",
    "    for phrase in tests:\n",
    "        print(\"before:\\n{0}\\nafter: \\n{1}\\n\".format(phrase, swap_uni(phrase.split(' '), zad3_tags, zad3_mapper)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę\n",
      "after: \n",
      "['Angielski', 'Prokurator', 'zniweczył', 'na', 'niedźwiedziej', 'godzinie', 'nieraz', 'niezliczoną', 'rolę']\n",
      "\n",
      "before:\n",
      "Duży pies znalazł w śmieciach prawdziwy skarb\n",
      "after: \n",
      "['Wrześniowy', 'jelonek', 'powitał', 'w', 'latach', 'pylasty', 'domiar']\n",
      "\n",
      "before:\n",
      "Pierwsze co zrobił po tym jak wstał to napił się ciepłej herbaty\n",
      "after: \n",
      "['Pierwsze', 'co', 'ukazał', 'na', 'tym', 'jak', 'został', 'to', 'wyjął', 'się', 'salezjańskiej', 'szkoły']\n",
      "\n",
      "before:\n",
      "Marcin rzekł zmarkniałym głosem, jakie zadanka znowu\n",
      "after: \n",
      "['Palacz', 'znalazł', 'wystarczy', 'zabraknie', 'jakie', 'biura', 'towarzysko']\n",
      "\n",
      "before:\n",
      "Zleziono ostatnio jakieś zdania do napisania\n",
      "after: \n",
      "['Wyzłośliwiono', 'dzisiaj', 'takie', 'podejścia', 'przeciwko', 'życia']\n",
      "\n",
      "before:\n",
      "Wczoraj wieczorem zauważyłem dużego dzika w pobliskim lasku\n",
      "after: \n",
      "['Wielokrotnie', 'wywiadem', 'wybrałem', 'ostatniego', 'żółwia', 'w', 'własnym', 'zamachu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę\",\n",
    "    \"Duży pies znalazł w śmieciach prawdziwy skarb\",\n",
    "    \"Pierwsze co zrobił po tym jak wstał to napił się ciepłej herbaty\",\n",
    "    \"Marcin rzekł zmarkniałym głosem, jakie zadanka znowu\",\n",
    "    \"Zleziono ostatnio jakieś zdania do napisania\",\n",
    "    \"Wczoraj wieczorem zauważyłem dużego dzika w pobliskim lasku\"\n",
    "]\n",
    "test_uniswap(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digram(file='../train data/2grams', k=10):\n",
    "    dictionary = defaultdict(list)\n",
    "    with open(file, 'r', encoding='utf8') as base_vectors_lines:\n",
    "        for line in base_vectors_lines:\n",
    "            line = line.strip().lower()\n",
    "            line = line.split(' ')\n",
    "            if int(line[0]) >= k:\n",
    "                dictionary[line[1]].append((line[2], int(line[0])))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def tag_mapper_digram(tags, digrams):\n",
    "    mapper = defaultdict(list)\n",
    "    for start in digrams:\n",
    "        if start not in tags:\n",
    "            continue\n",
    "        for end, weight in digrams[start]:\n",
    "            if end not in tags:\n",
    "                continue\n",
    "            tag1 = tags[start]\n",
    "            tag2 = tags[end]\n",
    "            mapper[(tag1, tag2)].append(((start, end), weight))\n",
    "    return mapper\n",
    "\n",
    "zad4_di = load_digram()\n",
    "zad4_mapper = tag_mapper_digram(zad3_tags, zad4_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_swap(words, tags, reversed_tags, reversed_bi_tags, bigrams):\n",
    "    new_phrase = []\n",
    "    new_phrase.append(swap_uni(words[0].lower().split(' '), zad3_tags, zad3_mapper)[0].capitalize())\n",
    "    \n",
    "    for start, end in zip(words, words[1:]):\n",
    "        upper = end[0].isupper()\n",
    "        start = start.lower()\n",
    "        end = end.lower()\n",
    "        \n",
    "        end_tag = tags[end] if end in tags else find_tag(reversed_tags, end)\n",
    "        \n",
    "        total = 0\n",
    "        sub = '#'\n",
    "        if start in bigrams:\n",
    "            for extension, weight in bigrams[start]:\n",
    "                if extension in tags and tags[extension] == end_tag:\n",
    "                    total += weight\n",
    "\n",
    "            if total > 0:\n",
    "                choice = rnd.randint(0, total)\n",
    "\n",
    "            total = 0\n",
    "            for extension, weight in bigrams[start]:\n",
    "                if extension in tags and tags[extension] == end_tag:\n",
    "                    total += weight\n",
    "                    if choice <= total:\n",
    "                        sub = extension\n",
    "                        break\n",
    "        if total == 0:\n",
    "            start = new_phrase[-1]\n",
    "            if start in bigrams:\n",
    "                for extension, weight in bigrams[start]:\n",
    "                    if extension in tags and tags[extension] == end_tag:\n",
    "                        total += weight\n",
    "\n",
    "                if total > 0:\n",
    "                    choice = rnd.randint(0, total)\n",
    "\n",
    "                total = 0\n",
    "                for extension, weight in bigrams[start]:\n",
    "                    if extension in tags and tags[extension] == end_tag:\n",
    "                        total += weight\n",
    "                        if choice <= total:\n",
    "                            sub = extension\n",
    "                            break\n",
    "\n",
    "        if total == 0:\n",
    "            new_phrase.append(\"|\")\n",
    "            sub = swap_uni(end.split(' '), zad3_tags, zad3_mapper)[0]\n",
    "        new_phrase.append(sub if not upper else sub.capitalize())\n",
    "\n",
    "    return new_phrase\n",
    "\n",
    "\n",
    "def test_diswap(tests):\n",
    "    for phrase in tests:\n",
    "        print(\"before:\\n{0}\\nafter: \\n{1}\\n\".format(phrase, bi_swap(phrase.split(' '), zad3_tags, zad3_mapper, zad4_mapper, zad4_di)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę\n",
      "after: \n",
      "['Drobny', 'Chłopczyk', '|', 'wskazał', 'w', 'niedalekiej', 'liczbie', 'około', '|', 'ukrytą', '|', 'poprawkę']\n",
      "\n",
      "before:\n",
      "Duży pies znalazł w śmieciach prawdziwy skarb\n",
      "after: \n",
      "['Tradycyjny', 'pies', 'zaatakował', 'w', 'latach', '|', 'pszeniczny', 'popis']\n",
      "\n",
      "before:\n",
      "Pierwsze co zrobił po tym jak wstał to napił się ciepłej herbaty\n",
      "after: \n",
      "['Pierwsze', 'co', 'przekazał', 'na', 'tym', 'jak', 'został', '|', 'to', 'ujął', 'się', 'polskiej', 'wody']\n",
      "\n",
      "before:\n",
      "Marcin rzekł zmarkniałym głosem, jakie zadanka znowu\n",
      "after: \n",
      "['Chłopak', '|', 'powiedział', '|', 'pozwoli', '|', 'zechce', '|', 'które', 'wrażenia', '|', 'sztywno']\n",
      "\n",
      "before:\n",
      "Zleziono ostatnio jakieś zdania do napisania\n",
      "after: \n",
      "['Przystąpiono', '|', 'znacznie', 'jakieś', 'źródła', 'wśród', 'biurka']\n",
      "\n",
      "before:\n",
      "Wczoraj wieczorem zauważyłem dużego dzika w pobliskim lasku\n",
      "after: \n",
      "['Chwilowo', 'wieczorem', 'wróciłem', '|', 'strategicznego', '|', 'jelenia', 'w', 'poprzednim', 'placu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_diswap(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(word, k):\n",
    "    RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ\\-]+')\n",
    "    word = word.lower()\n",
    "    if word not in zad4_di:\n",
    "        print('Unique word: {1}'.format(k))\n",
    "        return []\n",
    "    result = []\n",
    "    count = sum([v for _, v in zad4_di[word]])\n",
    "    uni_count = sum([zad3_uni[c] for c in zad3_uni])\n",
    "    for w, num in zad4_di[word]:\n",
    "        if w not in zad3_uni:\n",
    "            continue\n",
    "        res = re.findall(RE, w)\n",
    "        if len(res):\n",
    "            w = res[0]\n",
    "        if any([v == w for v, _ in result]):\n",
    "            continue\n",
    "        result.append((w, np.log((num / count) / ((zad3_uni[word] / uni_count) * (zad3_uni[w] / uni_count)))))\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "    return result[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"kot\", \"pies\", \"Polityk\", \"student\", \"doktor\", \"samochód\", \"praca\", \"kula\"]\n",
    "\n",
    "P1 = []\n",
    "P2 = []\n",
    "P3 = []\n",
    "for w in test_words:\n",
    "    P1.append(PPMI(w, 10))\n",
    "    P2.append(PPMI_ssc(w, 10))\n",
    "    P3.append(PPMI_T(w, 10))\n",
    "merged = [(w, list(zip(e1, e2, e3)))for w, e1, e2, e3 in zip(test_words, P1, P2, P3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kot',\n",
      "  [(('napłakał', 24.72022418679312), ('napłakał', 15689.225826636533), ('przybora', 22.12508170698412)),\n",
      "   (('przybora', 22.12508170698412), ('przybora', 15688.560193068817), ('napłakał', 12.966420183097531)),\n",
      "   (('domowy', 19.325993853693088), ('domowy', 15677.498209897412), ('domowy', 11.322987245716453)),\n",
      "   (('wskoczył', 19.249385880237284), ('wskoczył', 15676.5603508274), ('iwan', 9.826378611889014)),\n",
      "   (('przebiegł', 18.935915604836296), ('przebiegł', 15671.87844035071), ('wskoczył', 7.495581876541694)),\n",
      "   (('iwan', 18.65717014235092), ('iwan', 15666.285460523497), ('przebiegł', 2.498328300288673)),\n",
      "   (('leży', 16.258266431211688), ('leży', 15436.119675100197), ('jest', 0.30667368904797243)),\n",
      "   (('musi', 14.711691525477544), ('musi', 14500.61388179607), ('był', -0.7921593317136839)),\n",
      "   (('miał', 14.282371937536183), ('miał', 13863.23860749115), ('na', -1.126524754286951)),\n",
      "   (('ma', 14.216625287825083), ('ma', 13739.150202051314), ('w', -1.1619817957277299))]),\n",
      " ('pies',\n",
      "  [(('olejemy', 21.70176720187256), ('olejemy', 15688.833790130051), ('pawłowa', 19.073278823606532)),\n",
      "   (('pogrzebany', 21.489543540672372), ('pogrzebany', 15688.731323412085), ('myśliwski', 8.964729754533773)),\n",
      "   (('tropiący', 20.93962714982566), ('tropiący', 15688.327839350364), ('pogrzebany', 8.660274707694358)),\n",
      "   (('pogryzł', 19.85343738115611), ('pogryzł', 15686.462334578868), ('pogryzł', 8.52860908895533)),\n",
      "   (('zaszczekał', 19.729612031043764), ('zaszczekał', 15686.090426552846), ('policyjny', 8.177043610059178)),\n",
      "   (('szczekał', 19.533332121999102), ('szczekał', 15685.399450777399), ('tropiący', 7.8557160984740655)),\n",
      "   (('gończy', 19.383433751910374), ('gończy', 15684.772517959933), ('olejemy', 7.5778821039015085)),\n",
      "   (('pawłowa', 19.073278823606532), ('pawłowa', 15683.134585735652), ('ugryzł', 7.223660367289392)),\n",
      "   (('ugryzł', 18.54848865949017), ('ugryzł', 15678.896024882824), ('rasy', 6.483293315841239)),\n",
      "   (('myśliwski', 18.376388656767755), ('myśliwski', 15676.946450103364), ('szczekał', 6.24265250363057))]),\n",
      " ('Polityk',\n",
      "  [(('prawicy', 18.306191839255717), ('prawicy', 15651.423155144634), ('pis', 17.556616161239816)),\n",
      "   (('platformy', 17.71085994371281), ('platformy', 15620.624536284018), ('sld', 17.195970147024074)),\n",
      "   (('pis', 17.556616161239816), ('pis', 15609.175439421197), ('platformy', 9.11859801891017)),\n",
      "   (('sojuszu', 17.482028246315437), ('sojuszu', 15602.972113017071), ('partii', 7.332808602299945)),\n",
      "   (('sld', 17.195970147024074), ('sld', 15574.391119211406), ('unii', 6.847962835813391)),\n",
      "   (('powinien', 16.73304277189076), ('powinien', 15506.757203065756), ('sojuszu', 6.393433082230802)),\n",
      "   (('partii', 16.511628974609962), ('partii', 15461.519062135929), ('prawicy', 4.892214299283456)),\n",
      "   (('musi', 16.49190204817702), ('musi', 15456.984944458638), ('był', -0.32587468146931364)),\n",
      "   (('unii', 16.02678320812341), ('unii', 15319.415727466936), ('jest', -0.7638445208513147)),\n",
      "   (('ma', 14.52543678873518), ('ma', 14029.44124439708), ('po', -1.3695036540372065))]),\n",
      " ('student',\n",
      "  [(('politologii', 19.7947808163657), ('politologii', 15684.98839786215), ('network', 18.81940116812154)),\n",
      "   (('polonistyki', 19.69947063656138), ('polonistyki', 15684.559552890383), ('awf', 18.430820034782634)),\n",
      "   (('filologii', 18.81940116812154), ('filologii', 15677.903133111287), ('v', 17.233860056662724)),\n",
      "   (('network', 18.81940116812154), ('network', 15677.902942036471), ('iv', 16.743730984247385)),\n",
      "   (('politechniki', 18.710958330843898), ('politechniki', 15676.604322799787), ('iii', 16.44935779793054)),\n",
      "   (('socjologii', 18.601423160108226), ('socjologii', 15675.133336493831), ('ii', 15.434783089660638)),\n",
      "   (('dziennikarstwa', 18.438833611635104), ('dziennikarstwa', 15672.635479781), ('of', 15.118099194009048)),\n",
      "   (('awf', 18.430820034782634), ('awf', 15672.503945480166), ('polonistyki', 11.558940417047266)),\n",
      "   (('informatyki', 18.166819867341047), ('informatyki', 15667.434040415592), ('politechniki', 10.570428111329784)),\n",
      "   (('medycyny', 17.8909134599402), ('medycyny', 15660.494897855455), ('politologii', 10.392392279474617))]),\n",
      " ('doktor',\n",
      "  [(('folklorystyki', 22.466876200892546), ('folklorystyki', 15689.038546075177), ('diwro', 21.38768199808154)),\n",
      "   (('habilitowanej', 21.65594598467622), ('habilitowanej', 15688.739984028205), ('pogonowski', 21.125317733614047)),\n",
      "   (('habilitowany', 21.557505911862968), ('habilitowany', 15688.687468358286), ('judym', 21.00535841853507)),\n",
      "   (('diwro', 21.38768199808154), ('diwro', 15688.575001159566), ('lewada', 20.962798804116275)),\n",
      "   (('pogonowski', 21.125317733614047), ('pogonowski', 15688.364294273879), ('bloth', 20.86748862431195)),\n",
      "   (('wiernik', 21.12186349874596), ('wiernik', 15688.36158389136), ('krantz', 20.8196979604756)),\n",
      "   (('judym', 21.00535841853507), ('judym', 15688.248156544876), ('lasarius', 20.477290988334573)),\n",
      "   (('lewada', 20.962798804116275), ('lewada', 15688.20458636353), ('latkowski', 20.40318301618085)),\n",
      "   (('bloth', 20.86748862431195), ('bloth', 15688.095989974807), ('jekyll', 20.394814766510336)),\n",
      "   (('krantz', 20.8196979604756), ('krantz', 15688.038392554883), ('jańczak', 20.29496943154062))]),\n",
      " ('samochód',\n",
      "  [(('ciężarowy', 19.20567132768468), ('ciężarowy', 15687.54549524312), ('audi', 15.993463840690046)),\n",
      "   (('dostawczy', 19.151183142400612), ('dostawczy', 15687.43986343538), ('vw', 15.925813297881687)),\n",
      "   (('osobowy', 19.029047792005365), ('osobowy', 15687.229959007118), ('daewoo', 15.351371601485255)),\n",
      "   (('pożarniczy', 18.94716063253318), ('pożarniczy', 15687.01987185658), ('bmw', 14.957176085635322)),\n",
      "   (('gaśniczy', 18.773888911259142), ('gaśniczy', 15686.594138929268), ('zniknął', 14.338420941240098)),\n",
      "   (('zygi', 18.632810312999236), ('zygi', 15686.186318064741), ('ciężarowy', 12.932376966278913)),\n",
      "   (('terenowy', 17.54915411548478), ('terenowy', 15680.143857755967), ('osobowy', 12.755753430599597)),\n",
      "   (('zaparkowany', 17.341533776762716), ('zaparkowany', 15678.035341302579), ('terenowy', 11.275859754079011)),\n",
      "   (('zastępczy', 17.283371044268502), ('zastępczy', 15677.361879173852), ('służbowy', 10.95074873462152)),\n",
      "   (('służbowy', 17.224043096027287), ('służbowy', 15676.634837115478), ('zygi', 10.92475587131954))]),\n",
      " ('praca',\n",
      "  [(('nakładcza', 19.96911444372935), ('nakładcza', 15688.694956164463), ('wre', 17.766963748057073)),\n",
      "   (('magisterska', 19.11027628660328), ('magisterska', 15687.90910546647), ('pt', 16.46785627950427)),\n",
      "   (('zmianowa', 19.02650640353782), ('zmianowa', 15687.783789435847), ('nakładcza', 12.931886014481865)),\n",
      "   (('dyplomowa', 18.99030335487386), ('dyplomowa', 15687.731939443749), ('magisterska', 12.073047857355796)),\n",
      "   (('doktorska', 18.4268852809887), ('doktorska', 15686.558780421186), ('zmianowa', 11.989277974290335)),\n",
      "   (('zespołowa', 17.947566880468415), ('zespołowa', 15684.887340084155), ('dyplomowa', 11.953074925626375)),\n",
      "   (('biurowa', 17.927894114869712), ('biurowa', 15684.79869277105), ('doktorska', 11.389656851741215)),\n",
      "   (('zbiorowa', 17.795998045585918), ('zbiorowa', 15684.175602971325), ('zespołowa', 10.91033845122093)),\n",
      "   (('zdalna', 17.77188986639313), ('zdalna', 15684.041864451958), ('biurowa', 10.890665685622228)),\n",
      "   (('wre', 17.766963748057073), ('wre', 15684.017033405538), ('zbiorowa', 10.758769616338434))]),\n",
      " ('kula',\n",
      "  [(('armatnia', 24.175814039248145), ('armatnia', 15689.087673762868), ('armatnia', 18.509841678446463)),\n",
      "   (('przeszyła', 23.012663229442467), ('przeszyła', 15688.671210919305), ('ziemska', 16.98881472335642)),\n",
      "   (('ziemska', 22.654787084158098), ('ziemska', 15688.411131209792), ('przeszyła', 14.326151572685008)),\n",
      "   (('ugodziła', 22.31899535113579), ('ugodziła', 15688.064369648695), ('ugodziła', 13.632483694378331)),\n",
      "   (('przebiła', 22.22207745394337), ('przebiła', 15687.941645633768), ('przebiła', 13.535565797185912)),\n",
      "   (('trafiła', 19.890466576576497), ('trafiła', 15675.530799081382), ('trafiła', 11.203954919819038)),\n",
      "   (('przeszła', 19.3985971244072), ('przeszła', 15666.793220476773), ('ognia', 9.539536636847439)),\n",
      "   (('słońca', 19.050787699179796), ('słońca', 15657.440363282769), ('słońca', 9.226486174162313)),\n",
      "   (('ognia', 18.78279273723081), ('ognia', 15647.655485890158), ('u', 3.9782068442241325)),\n",
      "   (('u', 16.73898727589209), ('u', 15367.967497211219), ('w', 2.620961922065323))])]\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(width=200, depth=7)\n",
    "pp.pprint(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: kot\n",
      "[   ('napłakał', 24.72022418679312),\n",
      "    ('przybora', 22.12508170698412),\n",
      "    ('domowy', 19.325993853693088),\n",
      "    ('wskoczył', 19.249385880237284),\n",
      "    ('przebiegł', 18.935915604836296),\n",
      "    ('iwan', 18.65717014235092),\n",
      "    ('leży', 16.258266431211688),\n",
      "    ('musi', 14.711691525477544),\n",
      "    ('miał', 14.282371937536183),\n",
      "    ('ma', 14.216625287825083)]\n",
      "\n",
      "word: pies\n",
      "[   ('olejemy', 21.70176720187256),\n",
      "    ('pogrzebany', 21.489543540672372),\n",
      "    ('tropiący', 20.93962714982566),\n",
      "    ('pogryzł', 19.85343738115611),\n",
      "    ('zaszczekał', 19.729612031043764),\n",
      "    ('szczekał', 19.533332121999102),\n",
      "    ('gończy', 19.383433751910374),\n",
      "    ('pawłowa', 19.073278823606532),\n",
      "    ('ugryzł', 18.54848865949017),\n",
      "    ('myśliwski', 18.376388656767755)]\n",
      "\n",
      "word: Polityk\n",
      "[   ('prawicy', 18.306191839255717),\n",
      "    ('platformy', 17.71085994371281),\n",
      "    ('pis', 17.556616161239816),\n",
      "    ('sojuszu', 17.482028246315437),\n",
      "    ('sld', 17.195970147024074),\n",
      "    ('powinien', 16.73304277189076),\n",
      "    ('partii', 16.511628974609962),\n",
      "    ('musi', 16.49190204817702),\n",
      "    ('unii', 16.02678320812341),\n",
      "    ('ma', 14.52543678873518)]\n",
      "\n",
      "word: student\n",
      "[   ('politologii', 19.7947808163657),\n",
      "    ('polonistyki', 19.69947063656138),\n",
      "    ('filologii', 18.81940116812154),\n",
      "    ('network', 18.81940116812154),\n",
      "    ('politechniki', 18.710958330843898),\n",
      "    ('socjologii', 18.601423160108226),\n",
      "    ('dziennikarstwa', 18.438833611635104),\n",
      "    ('awf', 18.430820034782634),\n",
      "    ('informatyki', 18.166819867341047),\n",
      "    ('medycyny', 17.8909134599402)]\n",
      "\n",
      "word: doktor\n",
      "[   ('folklorystyki', 22.466876200892546),\n",
      "    ('habilitowanej', 21.65594598467622),\n",
      "    ('habilitowany', 21.557505911862968),\n",
      "    ('diwro', 21.38768199808154),\n",
      "    ('pogonowski', 21.125317733614047),\n",
      "    ('wiernik', 21.12186349874596),\n",
      "    ('judym', 21.00535841853507),\n",
      "    ('lewada', 20.962798804116275),\n",
      "    ('bloth', 20.86748862431195),\n",
      "    ('krantz', 20.8196979604756)]\n",
      "\n",
      "word: samochód\n",
      "[   ('ciężarowy', 19.20567132768468),\n",
      "    ('dostawczy', 19.151183142400612),\n",
      "    ('osobowy', 19.029047792005365),\n",
      "    ('pożarniczy', 18.94716063253318),\n",
      "    ('gaśniczy', 18.773888911259142),\n",
      "    ('zygi', 18.632810312999236),\n",
      "    ('terenowy', 17.54915411548478),\n",
      "    ('zaparkowany', 17.341533776762716),\n",
      "    ('zastępczy', 17.283371044268502),\n",
      "    ('służbowy', 17.224043096027287)]\n",
      "\n",
      "word: praca\n",
      "[   ('nakładcza', 19.96911444372935),\n",
      "    ('magisterska', 19.11027628660328),\n",
      "    ('zmianowa', 19.02650640353782),\n",
      "    ('dyplomowa', 18.99030335487386),\n",
      "    ('doktorska', 18.4268852809887),\n",
      "    ('zespołowa', 17.947566880468415),\n",
      "    ('biurowa', 17.927894114869712),\n",
      "    ('zbiorowa', 17.795998045585918),\n",
      "    ('zdalna', 17.77188986639313),\n",
      "    ('wre', 17.766963748057073)]\n",
      "\n",
      "word: kula\n",
      "[   ('armatnia', 24.175814039248145),\n",
      "    ('przeszyła', 23.012663229442467),\n",
      "    ('ziemska', 22.654787084158098),\n",
      "    ('ugodziła', 22.31899535113579),\n",
      "    ('przebiła', 22.22207745394337),\n",
      "    ('trafiła', 19.890466576576497),\n",
      "    ('przeszła', 19.3985971244072),\n",
      "    ('słońca', 19.050787699179796),\n",
      "    ('ognia', 18.78279273723081),\n",
      "    ('u', 16.73898727589209)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_PPMI(PPMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI_ssc(word, k):\n",
    "    RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ\\-]+')\n",
    "    word = word.lower()\n",
    "    if word not in zad4_di:\n",
    "        print('Unique word: {1}'.format(word))\n",
    "        return []\n",
    "    result = []\n",
    "    count = sum([v for _, v in zad4_di[word]])\n",
    "    uni_count = sum([zad3_uni[c] for c in zad3_uni])\n",
    "    for w, num in zad4_di[word]:\n",
    "        if w not in zad3_uni:\n",
    "            continue\n",
    "        res = re.findall(RE, w)\n",
    "        if len(res):\n",
    "            w = res[0]\n",
    "        if any([v == w for v, _ in result]):\n",
    "            continue\n",
    "        x = (num / uni_count)\n",
    "        result.append((w, (x - ((zad3_uni[word] / uni_count) * (zad3_uni[w] / uni_count))) / np.sqrt(np.power(x * (1.0 - x), 2) / uni_count)))\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "    return result[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: kot\n",
      "[   ('napłakał', 15689.225826636533),\n",
      "    ('przybora', 15688.560193068817),\n",
      "    ('domowy', 15677.498209897412),\n",
      "    ('wskoczył', 15676.5603508274),\n",
      "    ('przebiegł', 15671.87844035071),\n",
      "    ('iwan', 15666.285460523497),\n",
      "    ('leży', 15436.119675100197),\n",
      "    ('musi', 14500.61388179607),\n",
      "    ('miał', 13863.23860749115),\n",
      "    ('ma', 13739.150202051314)]\n",
      "\n",
      "word: pies\n",
      "[   ('olejemy', 15688.833790130051),\n",
      "    ('pogrzebany', 15688.731323412085),\n",
      "    ('tropiący', 15688.327839350364),\n",
      "    ('pogryzł', 15686.462334578868),\n",
      "    ('zaszczekał', 15686.090426552846),\n",
      "    ('szczekał', 15685.399450777399),\n",
      "    ('gończy', 15684.772517959933),\n",
      "    ('pawłowa', 15683.134585735652),\n",
      "    ('ugryzł', 15678.896024882824),\n",
      "    ('myśliwski', 15676.946450103364)]\n",
      "\n",
      "word: Polityk\n",
      "[   ('prawicy', 15651.423155144634),\n",
      "    ('platformy', 15620.624536284018),\n",
      "    ('pis', 15609.175439421197),\n",
      "    ('sojuszu', 15602.972113017071),\n",
      "    ('sld', 15574.391119211406),\n",
      "    ('powinien', 15506.757203065756),\n",
      "    ('partii', 15461.519062135929),\n",
      "    ('musi', 15456.984944458638),\n",
      "    ('unii', 15319.415727466936),\n",
      "    ('ma', 14029.44124439708)]\n",
      "\n",
      "word: student\n",
      "[   ('politologii', 15684.98839786215),\n",
      "    ('polonistyki', 15684.559552890383),\n",
      "    ('filologii', 15677.903133111287),\n",
      "    ('network', 15677.902942036471),\n",
      "    ('politechniki', 15676.604322799787),\n",
      "    ('socjologii', 15675.133336493831),\n",
      "    ('dziennikarstwa', 15672.635479781),\n",
      "    ('awf', 15672.503945480166),\n",
      "    ('informatyki', 15667.434040415592),\n",
      "    ('medycyny', 15660.494897855455)]\n",
      "\n",
      "word: doktor\n",
      "[   ('folklorystyki', 15689.038546075177),\n",
      "    ('habilitowanej', 15688.739984028205),\n",
      "    ('habilitowany', 15688.687468358286),\n",
      "    ('diwro', 15688.575001159566),\n",
      "    ('pogonowski', 15688.364294273879),\n",
      "    ('wiernik', 15688.36158389136),\n",
      "    ('judym', 15688.248156544876),\n",
      "    ('lewada', 15688.20458636353),\n",
      "    ('bloth', 15688.095989974807),\n",
      "    ('krantz', 15688.038392554883)]\n",
      "\n",
      "word: samochód\n",
      "[   ('ciężarowy', 15687.54549524312),\n",
      "    ('dostawczy', 15687.43986343538),\n",
      "    ('osobowy', 15687.229959007118),\n",
      "    ('pożarniczy', 15687.01987185658),\n",
      "    ('gaśniczy', 15686.594138929268),\n",
      "    ('zygi', 15686.186318064741),\n",
      "    ('terenowy', 15680.143857755967),\n",
      "    ('zaparkowany', 15678.035341302579),\n",
      "    ('zastępczy', 15677.361879173852),\n",
      "    ('służbowy', 15676.634837115478)]\n",
      "\n",
      "word: praca\n",
      "[   ('nakładcza', 15688.694956164463),\n",
      "    ('magisterska', 15687.90910546647),\n",
      "    ('zmianowa', 15687.783789435847),\n",
      "    ('dyplomowa', 15687.731939443749),\n",
      "    ('doktorska', 15686.558780421186),\n",
      "    ('zespołowa', 15684.887340084155),\n",
      "    ('biurowa', 15684.79869277105),\n",
      "    ('zbiorowa', 15684.175602971325),\n",
      "    ('zdalna', 15684.041864451958),\n",
      "    ('wre', 15684.017033405538)]\n",
      "\n",
      "word: kula\n",
      "[   ('armatnia', 15689.087673762868),\n",
      "    ('przeszyła', 15688.671210919305),\n",
      "    ('ziemska', 15688.411131209792),\n",
      "    ('ugodziła', 15688.064369648695),\n",
      "    ('przebiła', 15687.941645633768),\n",
      "    ('trafiła', 15675.530799081382),\n",
      "    ('przeszła', 15666.793220476773),\n",
      "    ('słońca', 15657.440363282769),\n",
      "    ('ognia', 15647.655485890158),\n",
      "    ('u', 15367.967497211219)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_PPMI(PPMI_ssc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum = defaultdict(int)\n",
    "for w in zad3_uni:\n",
    "    if w in zad3_tags:\n",
    "        t_sum[zad3_tags[w]] += 1\n",
    "\n",
    "def PPMI_tags(word, w2):\n",
    "    result = []\n",
    "\n",
    "    t1 = zad3_tags[word]\n",
    "    \n",
    "    count = sum([sum([x[1] for x in zad4_mapper[w]]) for w in zad4_mapper])\n",
    "    t_count = sum([1 for c in zad3_tags])\n",
    "    if w2 not in zad3_tags:\n",
    "        return 0\n",
    "    t2 = zad3_tags[w2]\n",
    "    if (t1, t2) not in zad4_mapper:\n",
    "        return 0\n",
    "    weight = sum([x[1] for x in zad4_mapper[(t1, t2)]])\n",
    "    return (np.log((weight / count) / (t_sum[t1] / t_count) * (t_sum[t2] / t_count)))\n",
    "\n",
    "def PPMI_T(word, k):\n",
    "    RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ]+')\n",
    "    word = word.lower()\n",
    "    if word not in zad4_di:\n",
    "        print('Unique word: {1}'.format(k))\n",
    "        return []\n",
    "    result = []\n",
    "    count = sum([v for _, v in zad4_di[word]])\n",
    "    uni_count = sum([zad3_uni[c] for c in zad3_uni])\n",
    "    for w, num in zad4_di[word]:\n",
    "        if w not in zad3_uni:\n",
    "            continue\n",
    "        res = re.findall(RE, w)\n",
    "        if len(res):\n",
    "            w = res[0]\n",
    "        else:\n",
    "            continue\n",
    "        if any([v == w for v, _ in result]):\n",
    "            continue\n",
    "        r = PPMI_tags(word, w)\n",
    "        result.append((w, np.log((num / count) / ((zad3_uni[word] / uni_count) * (zad3_uni[w] / uni_count))) + r))\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "    return result[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('przybora', 22.12508170698412),\n",
       " ('napłakał', 12.966420183097531),\n",
       " ('domowy', 11.322987245716453),\n",
       " ('iwan', 9.826378611889014),\n",
       " ('wskoczył', 7.495581876541694),\n",
       " ('przebiegł', 2.498328300288673),\n",
       " ('jest', 0.30667368904797243),\n",
       " ('był', -0.7921593317136839),\n",
       " ('na', -1.126524754286951),\n",
       " ('w', -1.1619817957277299)]"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI_T(\"kot\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_lines = []\n",
    "RE = re.compile(r'[a-zA-ZęóąśłżźćńĘÓĄŚŁŻŹĆŃ\\-]+')\n",
    "with open('../train data/pan-tadeusz.txt', 'r', encoding='utf8') as pt:\n",
    "    nf = False\n",
    "    for line in pt:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if not nf:\n",
    "            if line.strip().split(' ')[0] == 'Litwo!':\n",
    "                nf = True\n",
    "        if nf:\n",
    "            if line.strip()[0] == '-':\n",
    "                break\n",
    "            else:\n",
    "                PT_lines.append([word.lower() for word in re.findall(RE, line)])\n",
    "                \n",
    "vovels = ['a', 'ą', 'e', 'ę', 'i', 'o', 'u', 'y', 'ó']\n",
    "def accent(sentence):\n",
    "    res = []\n",
    "    for w in sentence:\n",
    "        c = 0\n",
    "        for c1, c2 in zip('^' + w, w):\n",
    "            if c2 in vovels and c1 != 'i':\n",
    "                c += 1\n",
    "        res.append(c)\n",
    "    return res\n",
    "\n",
    "word_to_tag = {}\n",
    "tag_to_word = defaultdict(list)\n",
    "with open('../train data/supertags.txt', 'r', encoding='utf8') as file:\n",
    "    for line in file:\n",
    "        word, tag = line.split()\n",
    "        word_to_tag[word] = tag\n",
    "        tag_to_word[tag].append(word)\n",
    "\n",
    "def get_rhyme(word):\n",
    "    for i in range(1, len(word)):\n",
    "        if accent([word[-i:]])[0] == 2:\n",
    "            return word[-i:]\n",
    "    return None\n",
    "\n",
    "def single_verset(sentences):\n",
    "    idx = rnd.randint(1, len(sentences) - 2)\n",
    "    rhyme = get_rhyme(sentences[idx][-1])\n",
    "\n",
    "    if get_rhyme(sentences[idx-1][-1]) == rhyme:\n",
    "        return sentences[idx-1], sentences[idx]\n",
    "    elif get_rhyme(sentences[idx+1][-1]) == rhyme:\n",
    "        return sentences[idx], sentences[idx+1]\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_tags(w):\n",
    "    if w in word_to_tag and rnd.random() > 0.05:\n",
    "        return tag_to_word[word_to_tag[w]]\n",
    "    elif ('#' + w)[-3:] in word_to_tag and rnd.random() > 0.05:\n",
    "        return tag_to_word[word_to_tag[('#' + w)[-3:]]]\n",
    "    else:\n",
    "        return tag_to_word[rnd.choice(list((word_to_tag.values())))]\n",
    "\n",
    "def random_werse():    \n",
    "    while True:\n",
    "        sentence1 = []\n",
    "        sentence2 = []\n",
    "        start = rnd.randint(0, len(PT_lines)-2)\n",
    "        w1, w2 = single_verset(PT_lines)\n",
    "        if not w1:\n",
    "            continue\n",
    "    \n",
    "        if get_rhyme(w1[-1]) != get_rhyme(w2[-1]):\n",
    "            continue    \n",
    "\n",
    "        a1, a2 = accent(w1), accent(w2)\n",
    "        \n",
    "        t1 = [same_tags(w) for w in w1]\n",
    "        t2 = [same_tags(w) for w in w2]\n",
    "\n",
    "        if None in t1 or None in t2:\n",
    "            continue\n",
    "        \n",
    "        candidates1 = [[c for c in candidate if accent([c])[0] == a] for candidate, a in zip(t1, a1)]\n",
    "        candidates2 = [[c for c in candidate if accent([c])[0] == a] for candidate, a in zip(t2, a2)]\n",
    "\n",
    "        cn1 = []\n",
    "        cn2 = []\n",
    "        \n",
    "        for w in candidates1[-1]:\n",
    "            for c2 in candidates2[-1]:\n",
    "                if get_rhyme(w) == get_rhyme(c2):\n",
    "                    cn1.append(w)\n",
    "                    break\n",
    "        for w in candidates2[-1]:\n",
    "            for c2 in candidates1[-1]:\n",
    "                if get_rhyme(w) == get_rhyme(c2):\n",
    "                    cn2.append(w)\n",
    "                    break\n",
    "\n",
    "        candidates1[-1] = cn1\n",
    "        candidates2[-1] = cn2\n",
    "\n",
    "        if [] in candidates1 or [] in candidates2:\n",
    "            continue    \n",
    "            \n",
    "        weights1 = [[zad3_uni[w] if w in zad3_uni else 2 for w in l] for l in candidates1]\n",
    "        weights2 = [[zad3_uni[w] if w in zad3_uni else 2 for w in l] for l in candidates2]\n",
    "\n",
    "        for _ in range(1000):\n",
    "            res1 = []\n",
    "            res2 = []\n",
    "            for wl, weights in zip(candidates1, weights1):\n",
    "                k = []\n",
    "                ww = []\n",
    "                for x, w2 in zip(wl, weights):\n",
    "                    if x in res1 or x in res2:\n",
    "                        break\n",
    "                    else:\n",
    "                        k.append(x)\n",
    "                        ww.append(w2)\n",
    "                if not k:\n",
    "                    break\n",
    "                res1.append(rnd.choices(k, weights=ww)[0])\n",
    "    \n",
    "            for wl, weights in zip(candidates2, weights2):\n",
    "                k = []\n",
    "                ww = []\n",
    "                for x, w2 in zip(wl, weights):\n",
    "                    if x in res2 or x in res1:\n",
    "                        break\n",
    "                    else:\n",
    "                        k.append(x)\n",
    "                        ww.append(w2)\n",
    "                if not k:\n",
    "                    break\n",
    "\n",
    "                res2.append(rnd.choices(k, weights=ww)[0])\n",
    "            \n",
    "            if len(res1) < len(candidates1) or len(res2) < len(candidates2):\n",
    "                continue\n",
    "            if get_rhyme(res1[-1]) != get_rhyme(res2[-1]):\n",
    "                continue\n",
    "            return \"{0}\\n{1}\\n\".format(\" \".join(res1), \" \".join(res2))\n",
    "        \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "czyli imprezy łamiąc telefony\n",
      "gdy zablokują wyświetlane strony\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obok na godnej drodze internat hrabiowski\n",
      "po ciemnej wziął psułoby pół i jarzembowski\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wprawdzie dała się mało sensem nabożniejsza\n",
      "bo zamknięta a obiekt oznacza i zmniejsza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dość nie dzieli ta śląska wyniszczenie babia\n",
      "a potem czegóż jeszcze zbył mi tak ten hrabia\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobak wciąż mówił a raz coraz więcej nosił\n",
      "aż budownictwo szkołą uporem poprosił\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dojrzawszy zadam miejsca rosną po moskali\n",
      "bezpośrednio ich maluch z twórcą kierowali\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i choć jej zamierzają służbowe siatkarki\n",
      "nie mogła w który sposób wrócić gospodarki\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "że stali daną siebie oba urzędnicy\n",
      "i przez naszej wybili razem kamienicy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(random_werse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zad 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PPMI_ucount = sum([zad3_uni[c] for c in zad3_uni])\n",
    "\n",
    "def PPMI_value(w1, w2, sb=0):\n",
    "    if sb == 3:\n",
    "        return 0.0\n",
    "    if w1 not in zad3_uni or w2 not in zad3_uni:\n",
    "        return PPMI_value(superbase_mapping([w1])[0], superbase_mapping(w2)[0], sb + 1)\n",
    "    p = False\n",
    "    for w, _ in zad4_di[w1]:\n",
    "        if w == w2:\n",
    "            p = True\n",
    "            break\n",
    "    if not p:\n",
    "        return PPMI_value(w1, superbase_mapping([w2])[0], sb + 1)\n",
    "    num = 0\n",
    "    PPMI_count = sum([v for _, v in zad4_di[w1]])\n",
    "    for w, n in zad4_di[w1]:\n",
    "        if w == w2:\n",
    "            num = n\n",
    "            break\n",
    "    return np.log((num / PPMI_count) / ((zad3_uni[w1] / PPMI_ucount) * (zad3_uni[w2] / PPMI_ucount)))\n",
    "\n",
    "def PPMI_value_sentence(sentence):\n",
    "    score = 0.0\n",
    "    for w1, w2 in zip(sentence, sentence[1:]):\n",
    "        score += PPMI_value(w1, w2)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_tags(w):\n",
    "    if w in word_to_tag and rnd.random() > 0.05:\n",
    "        return tag_to_word[word_to_tag[w]]\n",
    "    elif ('#' + w)[-3:] in word_to_tag and rnd.random() > 0.05:\n",
    "        return tag_to_word[word_to_tag[('#' + w)[-3:]]]\n",
    "    else:\n",
    "        return tag_to_word[rnd.choice(list((word_to_tag.values())))]\n",
    "\n",
    "def random_werse_PPMI():    \n",
    "    while True:\n",
    "        sentence1 = []\n",
    "        sentence2 = []\n",
    "        start = rnd.randint(0, len(PT_lines)-2)\n",
    "        w1, w2 = single_verset(PT_lines)\n",
    "        if not w1:\n",
    "            continue\n",
    "            \n",
    "        if get_rhyme(w1[-1]) != get_rhyme(w2[-1]):\n",
    "            continue\n",
    "        \n",
    "        a1, a2 = accent(w1), accent(w2)\n",
    "        \n",
    "        t1 = [same_tags(w) for w in w1]\n",
    "        t2 = [same_tags(w) for w in w2]\n",
    "\n",
    "        if None in t1 or None in t2:\n",
    "            continue\n",
    "        \n",
    "        candidates1 = [[c for c in candidate if accent([c])[0] == a] for candidate, a in zip(t1, a1)]\n",
    "        candidates2 = [[c for c in candidate if accent([c])[0] == a] for candidate, a in zip(t2, a2)]\n",
    "\n",
    "        cn1 = []\n",
    "        cn2 = []\n",
    "        \n",
    "        for w in candidates1[-1]:\n",
    "            for c2 in candidates2[-1]:\n",
    "                if get_rhyme(w) == get_rhyme(c2):\n",
    "                    cn1.append(w)\n",
    "                    break\n",
    "        for w in candidates2[-1]:\n",
    "            for c2 in candidates1[-1]:\n",
    "                if get_rhyme(w) == get_rhyme(c2):\n",
    "                    cn2.append(w)\n",
    "                    break\n",
    "\n",
    "        candidates1[-1] = cn1\n",
    "        candidates2[-1] = cn2\n",
    "\n",
    "        if [] in candidates1 or [] in candidates2:\n",
    "            continue    \n",
    "            \n",
    "        weights1 = [[zad3_uni[w] if w in zad3_uni else 2 for w in l] for l in candidates1]\n",
    "        weights2 = [[zad3_uni[w] if w in zad3_uni else 2 for w in l] for l in candidates2]\n",
    "\n",
    "        results_final1 = []\n",
    "        results_final2 = []\n",
    "        for _ in range(20000):\n",
    "            res1 = []\n",
    "            res2 = []\n",
    "            for wl, weights in zip(candidates1, weights1):\n",
    "                k = []\n",
    "                ww = []\n",
    "                for x, w2 in zip(wl, weights):\n",
    "                    if x in res1 or x in res2:\n",
    "                        break\n",
    "                    else:\n",
    "                        k.append(x)\n",
    "                        ww.append(w2)\n",
    "                if not k:\n",
    "                    break\n",
    "                res1.append(rnd.choices(k, weights=ww)[0])\n",
    "    \n",
    "            for wl, weights in zip(candidates2, weights2):\n",
    "                k = []\n",
    "                ww = []\n",
    "                for x, w2 in zip(wl, weights):\n",
    "                    if x in res2 or x in res1:\n",
    "                        break\n",
    "                    else:\n",
    "                        k.append(x)\n",
    "                        ww.append(w2)\n",
    "                if not k:\n",
    "                    break\n",
    "\n",
    "                res2.append(rnd.choices(k, weights=ww)[0])\n",
    "            \n",
    "            if len(res1) < len(candidates1) or len(res2) < len(candidates2):\n",
    "                continue\n",
    "            if get_rhyme(res1[-1]) != get_rhyme(res2[-1]):\n",
    "                continue\n",
    "            results_final1.append((res1, PPMI_value_sentence(res1)))\n",
    "            results_final2.append((res2, PPMI_value_sentence(res2)))\n",
    "        \n",
    "        if len(results_final1) < 5:\n",
    "            continue\n",
    "\n",
    "        res_final = []\n",
    "        bs = 0.0\n",
    "        for (r1, s1), (r2, s2) in zip(results_final1, results_final2):\n",
    "            res_final.append((s1 + s2, (r1, r2)))\n",
    "        res_final.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [\"score {0}\\n{1}\\n{2}\\n\\n\".format(x[0], \" \".join(x[1][0]), \" \".join(x[1][1])) for x in res_final[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 30.581140223145646\n",
      "źle stąd bo duchom jezus nas mdły odpoczynek\n",
      "kroćset trocin wyników wód przeszkód dożynek\n",
      "\n",
      "\n",
      "score 30.333791190656523\n",
      "dziś stąd bo diabłom jezus nas mdły wypoczynek\n",
      "kroćset perfum rodzajów tam siedzib dożynek\n",
      "\n",
      "\n",
      "score 24.570426620903177\n",
      "znów stąd bo ciskom jezus nas pstry wypoczynek\n",
      "kroćset dziejów tysięcy stron książek dożynek\n",
      "\n",
      "\n",
      "score 24.503410873358373\n",
      "dziś stąd bo koniom ojciec nas mdły odpoczynek\n",
      "kroćset dziejów obrazów tam osób dożynek\n",
      "\n",
      "\n",
      "score 23.21817862916079\n",
      "wraz stąd bo koniom anioł nas mdły wypoczynek\n",
      "kroćset swarów zespołów szkół zasad dożynek\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = random_werse_PPMI()\n",
    "for w in res:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 69.8817194313011\n",
      "była to katastrofa osiecka warszawa\n",
      "pod której miał pójść nawet nowy tekst ustawa\n",
      "\n",
      "\n",
      "score 63.16879810000797\n",
      "była to koalicja mołdawska warszawa\n",
      "po której miał pójść nieco własny rząd ustawa\n",
      "\n",
      "\n",
      "score 61.67844223024633\n",
      "była to sytuacja winnicka warszawa\n",
      "na której miał wejść dzisiaj cały sprzęt ustawa\n",
      "\n",
      "\n",
      "score 61.172181647032204\n",
      "była to propozycja balicka christina\n",
      "po której miał wyjść wreszcie każdy rząd rodzina\n",
      "\n",
      "\n",
      "score 58.17886660195677\n",
      "była to medycyna wileńska połowa\n",
      "na żadnej miał wejść nawet mały skok rozmowa\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = random_werse_PPMI()\n",
    "for w in res:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 58.00076940509564\n",
      "polski dzisiejszy nawet rada cześć kordiana\n",
      "poprawna i już wielkim znakiem obtrząsana\n",
      "\n",
      "\n",
      "score 56.54171839496032\n",
      "polski dzisiejszy trudno praca rzecz bogdana\n",
      "bezmyślna i już wielkim wpływem nazywana\n",
      "\n",
      "\n",
      "score 56.52177469230555\n",
      "polski powyższy przedtem kilka wieś szymona\n",
      "rozsądna i już wielkim hukiem ulubiona\n",
      "\n",
      "\n",
      "score 50.33166632029846\n",
      "drogi silniejszy nawet praca rzecz stefana\n",
      "ostatnia i już nowym typem obstawiana\n",
      "\n",
      "\n",
      "score 49.590862979220454\n",
      "drogi wczorajszy wczoraj hanna maź romana\n",
      "prawdziwa i już głównym celem obniżana\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = random_werse_PPMI()\n",
    "for w in res:\n",
    "    print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
